{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchtext import data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token for token in list(text)]\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenize,init_token = '<sos>' ,eos_token = '<eos>', pad_token='<pad>')\n",
    "fields = [('src',TEXT),('trg',TEXT)]\n",
    "\n",
    "train_data = data.TabularDataset.splits(path=\"./data/text/mai2000a_src_trg.tsv\", format='tsv', fields=[('source',TEXT),('target',TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジャズのアート化、コンサート化を推進したと近年かなり激しい批判が飛び交ったが、それが白人アーティストからだけ出ている点に、私は一抹の寂しさを覚える。\n",
      "ャズのアート化、コンサート化を推進したと近年かなり激しい批判が飛び交ったが、それが白人アーティストからだけ出ている点に、私は一抹の寂しさを覚える。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def make_target(src):\n",
    "    #trg = []\n",
    "    #for line in src:\n",
    "        #trg.append(list(line)[1:])\n",
    "    #return trg_words\n",
    "    return [line[1:] for line in src]\n",
    "\n",
    "input_data_path = \"./data/text/mai2000a_token.txt\"\n",
    "with  open(input_data_path, 'r', encoding='utf-8') as f:\n",
    "    input_lines = f.read().split('\\n')  # 行ごとのリストに\n",
    "train_src, val_src = train_test_split(input_lines, train_size=0.9)\n",
    "val_src, test_src = train_test_split(val_src, train_size=0.5)\n",
    "\n",
    "train_trg = make_target(train_src)\n",
    "val_trg = make_target(val_src)\n",
    "test_trg = make_target(test_src)\n",
    "print(val_src[0])\n",
    "print(val_trg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_list(src, trg, field):\n",
    "    fields = [('src', field), ('trg', field)]\n",
    "    examples =[]\n",
    "    for src_line, trg_line in zip(src, trg):\n",
    "        examples.append(data.Example.fromlist([src_line,trg_line], fields))\n",
    "    return data.Dataset(examples, fields)\n",
    "\n",
    "train_data = dataset_from_list(train_src, train_trg, TEXT)\n",
    "val_data = dataset_from_list(val_src, val_trg, TEXT)\n",
    "test_data = dataset_from_list(test_src, test_trg, TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書作成\n",
    "TEXT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "val_batch_size = 100\n",
    "batch_sizes = [batch_size, val_batch_size, val_batch_size]\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train_data, val_data, test_data), batch_sizes=batch_sizes, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(- math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, n_tokens, emb_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, emb_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, src):\n",
    "        output = self.embedding(src)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosisionalEncoder(nn.Module):\n",
    "    def __init__(self, n_input, dropout = 0.5):\n",
    "        super(PosisionalEncoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_input, dropout)\n",
    "        self.n_input = n_input\n",
    "    def forward(self, src):\n",
    "        src = src * math.sqrt(self.n_input)\n",
    "        output = self.pos_encoder(src)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_head, n_hidden, n_layers, dropout = 0.5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        encoder_layers = TransformerEncoderLayer(n_input, n_head, n_hidden, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "    \n",
    "    def _generate_square_subsequent_mask (self, sz):\n",
    "        mask = (torch.triu(torch.ones (sz, sz )) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, emb_size, n_tokens, dropout = 0.5):\n",
    "        super(LinearHead, self).__init__()\n",
    "        self.linear = nn.Linear(emb_size, n_tokens)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        output = self.linear(src)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_token, emb_size, n_head, n_hidden, n_layers, dropout = 0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = Embedder(n_tokens, emb_size)\n",
    "        self.pos_encoder = PosisionalEncoder(emb_size, dropout)\n",
    "        self.transformer = TransformerBlock(emb_size, n_head, n_hidden, n_layers, dropout)\n",
    "        self.linear = LinearHead(emb_size, n_tokens, dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.embedding(src)\n",
    "        output = self.pos_encoder(output)\n",
    "        output = self.transformer(output)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emb_size = 200 # embedding dimension\n",
    "n_hidden = 256 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_layers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "n_head = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch):\n",
    "    src = batch.src[:-1,]\n",
    "    trg = batch.trg\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バッチのテンソルサイズ：src:torch.Size([208, 20])|trg:torch.Size([208, 20])\n",
      "入力のテンソルサイズ： torch.Size([208, 20, 200])\n",
      "出力のテンソルサイズ： torch.Size([208, 20, 200])\n"
     ]
    }
   ],
   "source": [
    "emb = Embedder(n_tokens, emb_size).to(device)\n",
    "pos = PosisionalEncoder(emb_size, dropout).to(device)\n",
    "trf = TransformerBlock(emb_size, n_head, n_hidden, n_layers, dropout).to(device)\n",
    "lnh = LinearHead(emb_size, n_tokens, dropout).to(device)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "src, trg = get_batch(batch)\n",
    "src= src.to(device)\n",
    "trg = trg.to(device)\n",
    "print(\"バッチのテンソルサイズ：src:{}|trg:{}\".format(src.size(),trg.size()))\n",
    "output = emb(src)\n",
    "print(\"入力のテンソルサイズ：\", output.shape)\n",
    "output = pos(output)\n",
    "output = trf(output)\n",
    "#output = lnh(output)\n",
    "print(\"出力のテンソルサイズ：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(n_tokens, emb_size, n_head, n_hidden, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "import time\n",
    "\n",
    "def train_batch(model, iterator, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    lr = 5.0 # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for batch in iterator:\n",
    "        src, trg = get_batch(batch)\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        loss = criterion(output, trg)  # 損失を計算\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()  # lossの合計を更新\n",
    "    return total_loss / len(train_data)\n",
    "\n",
    "def train(model, iterator, optimizer):\n",
    "    model.train()\n",
    "    lr = 5.0 # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for batch in iterator:\n",
    "        src, trg = get_batch(batch)\n",
    "        for i in range(batch_size):\n",
    "            src = src[:,i]\n",
    "            trg = trg[:,i]\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, trg)  # 損失を計算\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()  # lossの合計を更新\n",
    "            \n",
    "    return total_loss / len(train_data)\n",
    "\n",
    "def evaluate_batch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            for i in range(batch_size):\n",
    "                src, trg = get_batch(batch, i)\n",
    "                output = model(src)\n",
    "                loss = criterion(output, trg).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src, trg = get_batch(batch)\n",
    "            for i in range(batch_size):\n",
    "                src = src[:,i]\n",
    "                trg = trg[:,i]\n",
    "                src = src.to(device)\n",
    "                trg = trg.to(device)\n",
    "                output = model(src)\n",
    "                loss += criterion(output, trg).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "def train_iterator(model, num_epochs):\n",
    "    print(\"---start---\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(model, train_iter, criterion)\n",
    "        val_loss = evaluate(model, val_iter, criterion)\n",
    "        print('-' * 89)\n",
    "        print('end epoch:{:3d}|time:{:5.2f}s'.format(epoch, (time.time - epoch_start_time)))\n",
    "        print('train loss:{:5.2f} | valid loss:{:5.2f}'.format(train_loss, val_loss))\n",
    "        print('-' * 89)\n",
    "        if epoch_loss < best_val_loss:\n",
    "            best_val_loss = epoch_loss\n",
    "            best_model = model\n",
    "        start_time = time.time()\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---start---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (149, 4118), got torch.Size([149])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-49bba0656877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m89\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-4ef9d882e345>\u001b[0m in \u001b[0;36mtrain_iterator\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m89\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-4ef9d882e345>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 損失を計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1832\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 1834\u001b[1;33m                 out_size, target.size()))\n\u001b[0m\u001b[0;32m   1835\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (149, 4118), got torch.Size([149])"
     ]
    }
   ],
   "source": [
    "train_iterator(model, 3)\n",
    "\n",
    "\n",
    "test_loss = evaluate(best_model, test_iter, criterion)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} '.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sentence):\n",
    "    outputs = model(sentence)\n",
    "    return outputs\n",
    "\n",
    "def testRandomly(model,n=10):\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            text = random.choice(test_src)\n",
    "            print('>'+text)\n",
    "            input_tokens = list(test_src)\n",
    "            input_tokens.insert(0,'<sos>')\n",
    "            input_tokens.append('<eos>')\n",
    "            outputs = test(model, input_tokens)\n",
    "            for j, output in enumerate(outputs):\n",
    "                if len(input_tokens) <= j: \n",
    "                    break\n",
    "                topv_list, topi_list = output[0].topk(5)\n",
    "                print(input_tokens[j] + \":\")\n",
    "                output_tokens = []\n",
    "                for index in topi_list:\n",
    "                    output_tokens.append(data.itos[index.item()])\n",
    "            print(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), \"./weights/transformer_\"+str(epochs)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
